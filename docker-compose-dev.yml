version: '3'

services:
  # Needed to run postgres as a non-root user. This runs as root and sets the
  # postgres data directory to be owned by the non-root user that will run
  # postgres.
  postgres-init:
    image: postgres:alpine
    container_name: dev-covsonar-db-init
    env_file:
      - conf/dev_db.env
      - conf/dev_django.env
    volumes:
      - ${EXTERNAL_POSTGRES_DATA}:/var/lib/postgresql/data
    entrypoint: /bin/sh -c
    command: [ "chown -R ${POSTGRES_UID_GID} /var/lib/postgresql/data" ]

  postgres:
    image: postgres:alpine
    container_name: dev-covsonar-db
    restart: unless-stopped
    user: "${POSTGRES_UID_GID}"
    command: -c config_file=/etc/postgresql/postgresql.conf
    volumes:
      # Have to check that the configuration is actually loaded. Right now, I'm following the same approach as the official PostgreSQL Docker
      - ./conf/customPostgresql.conf:/etc/postgresql/postgresql.conf
      - './conf/initdb/:/docker-entrypoint-initdb.d/'
      # This exact path inside the container needs to be mapped to a volume,
      # otherwise it will automatically be mapped to a volume inside of the
      # host's /var/lib/docker/volumes directory
      - ${EXTERNAL_POSTGRES_DATA}:/var/lib/postgresql/data
    env_file:
      - conf/dev_db.env
      - conf/dev_django.env
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    depends_on:
      postgres-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s

  dev-django:
    image: backend_dev:local # hint: must be built before, use "build_docker_dev.ps1"
    env_file:
      - conf/dev_django.env
      - conf/dev_secrets.env
    container_name: dev-covsonar-django
    restart: unless-stopped
    command: gunicorn  --workers 2 --threads 2 --bind 0.0.0.0:9080 covsonar_backend.wsgi:application --timeout 300
    # command: python manage.py runserver 0.0.0.0:9080
    # enable this and "django_extensions" in settings.dev to enable per request profiling
    # also create the ./profiler directory in project root
    # command: python manage.py runprofileserver --use-cprofile --prof-path=/code/profiler 0.0.0.0:9080
    volumes:
      - ./covsonar2_server/test/media:/covsonarmedia
      - ./covsonar2_server/test/static:/staticfiles
      - ${EXTERNAL_SONAR_LOGS}:${LOG_PATH}
      - ${EXTERNAL_SONAR_INPUT_LOGS}:/input-logs
      - ${EXTERNAL_SONAR_DATA_ENTRY_FOLDER}:${SONAR_DATA_ENTRY_FOLDER}
      - ${EXTERNAL_SONAR_DATA_PROCESSING_FOLDER}:${SONAR_DATA_PROCESSING_FOLDER}
      - ${EXTERNAL_SONAR_DATA_ARCHIVE}:${SONAR_DATA_ARCHIVE}
    depends_on:
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
    ports:
      # to access directly, normally can use localhost:8000 via the nginx proxy
      - "9080"

  dev-django-apscheduler:
    image: backend_dev:local # hint: must be built before, use "build_docker_dev.ps1"
    env_file:
      - conf/dev_django.env
      - conf/dev_secrets.env
    container_name: dev-covsonar-django-apscheduler
    restart: on-failure
    command: >
      bash -c "python manage.py runapscheduler"
    volumes:
      - .:/code
    depends_on:
      postgres:
        condition: service_healthy
      dev-django:
        condition: service_started

  backend-nginx:
    build:
      context: ./nginx
    volumes:
      - ./nginx/covsonar.conf:/etc/nginx/conf.d/default.conf
      - ./covsonar2_server/test/static:/staticfiles
      - ./covsonar2_server/test/media:/mediafiles
    ports:
      - "8000:8000"
    depends_on:
      - "dev-django"

  redis:
    image: redis:7
    container_name: dev-covsonar-cache
    restart: unless-stopped
    ports:
      - "6379"

  celery-workers:
    container_name: celery-workers
    image: backend_dev:local
    # The workers (concurrency, autoscale) are affected by the SAMPLE_BATCH_SIZE parameter in backend. For example, 
    # if the concurrency is set too high and the SAMPLE_BATCH_SIZE is too high,
    # this will not speed up the process. You need to lower SAMPLE_BATCH_SIZE into smaller chunks to gain the benefits
    # --autoscale max, min  means will have at least min and at most max concurrent worker subprocesses for a given worker instance.
    # command: celery -A covsonar_backend worker --loglevel DEBUG -Ofair  --autoscale=5,0 -E --max-tasks-per-child 15 --time-limit 180
    # --concurrency=N means will have exactly N worker subprocesses for your worker instance (meaning the worker instance can handle N conccurent tasks).
    command: celery -A covsonar_backend worker --loglevel ${LOG_LEVEL} -Ofair  --concurrency=4 -E --max-tasks-per-child 8 --time-limit 600 -n Bob
    volumes:
      - .:/code # working directory of import process
    env_file:
      - conf/dev_django.env
      - conf/dev_secrets.env
    depends_on:
      - redis
      - dev-django
      
  celery-monitor:
    container_name: celery-monitor
    image: backend_dev:local
    command: celery -A covsonar_backend flower --port=5555 --basic_auth=note:123456 --enable_events=False
    env_file:
      - conf/dev_django.env
      - conf/dev_secrets.env
    ports:
      - "5555:5555"
    depends_on:
      - redis
      - dev-django
      - celery-workers
